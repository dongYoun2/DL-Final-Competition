{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15e79da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu126\n",
      "CUDA available: True\n",
      "✓ F.scaled_dot_product_attention is available\n",
      "\n",
      "Available SDPA backends:\n",
      "  FLASH_ATTENTION: SDPBackend.FLASH_ATTENTION\n",
      "  EFFICIENT_ATTENTION: SDPBackend.EFFICIENT_ATTENTION\n",
      "  MATH: SDPBackend.MATH\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Check if Flash Attention backend is available (PyTorch 2.0+)\n",
    "if hasattr(torch.nn.functional, 'scaled_dot_product_attention'):\n",
    "    print(\"✓ F.scaled_dot_product_attention is available\")\n",
    "\n",
    "    # Check available backends\n",
    "    from torch.nn.attention import SDPBackend\n",
    "    print(f\"\\nAvailable SDPA backends:\")\n",
    "    print(f\"  FLASH_ATTENTION: {SDPBackend.FLASH_ATTENTION}\")\n",
    "    print(f\"  EFFICIENT_ATTENTION: {SDPBackend.EFFICIENT_ATTENTION}\")\n",
    "    print(f\"  MATH: {SDPBackend.MATH}\")\n",
    "else:\n",
    "    print(\"✗ scaled_dot_product_attention NOT available (need PyTorch 2.0+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b004e3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timm version: 1.0.22\n",
      "\n",
      "Model architecture: vit_base_patch8_224\n",
      "✓ Model created successfully\n",
      "\n",
      "Attention module type: Attention\n",
      "Attention class: timm.layers.attention.Attention\n",
      "fused_attn attribute: True\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "from hydra import initialize, compose\n",
    "\n",
    "print(f\"timm version: {timm.__version__}\")\n",
    "\n",
    "with initialize(version_base=None, config_path=\"configs\"):\n",
    "    cfg = compose(config_name=\"model/dino_v2\")\n",
    "    vit_cfg = cfg.model.vit\n",
    "\n",
    "    # First, let's see what parameters are accepted\n",
    "    print(f\"\\nModel architecture: {cfg.model.architecture}\")\n",
    "\n",
    "    # Try creating without attn_impl first\n",
    "    try:\n",
    "        model = timm.create_model(\n",
    "            cfg.model.architecture,\n",
    "            pretrained=False,\n",
    "            num_classes=0,\n",
    "            img_size=vit_cfg.image_size,\n",
    "            patch_size=vit_cfg.patch_size,\n",
    "            embed_dim=vit_cfg.embed_dim,\n",
    "            depth=vit_cfg.depth,\n",
    "            num_heads=vit_cfg.num_heads,\n",
    "            mlp_ratio=vit_cfg.mlp_ratio,\n",
    "            drop_path_rate=vit_cfg.drop_path_rate,\n",
    "        )\n",
    "        print(\"✓ Model created successfully\")\n",
    "\n",
    "        # Check what attention implementation is being used\n",
    "        if hasattr(model, 'blocks'):\n",
    "            first_block = model.blocks[0]\n",
    "            if hasattr(first_block, 'attn'):\n",
    "                attn = first_block.attn\n",
    "                print(f\"\\nAttention module type: {type(attn).__name__}\")\n",
    "                print(f\"Attention class: {attn.__class__.__module__}.{attn.__class__.__name__}\")\n",
    "\n",
    "                # Check if it has fused_attn attribute (indicates SDPA support)\n",
    "                if hasattr(attn, 'fused_attn'):\n",
    "                    print(f\"fused_attn attribute: {attn.fused_attn}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c25509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Testing Flash Attention with actual model\n",
      "================================================================================\n",
      "Enabled fused_attn for block\n",
      "Enabled fused_attn for block\n",
      "Enabled fused_attn for block\n",
      "Enabled fused_attn for block\n",
      "Enabled fused_attn for block\n",
      "Enabled fused_attn for block\n",
      "Enabled fused_attn for block\n",
      "Enabled fused_attn for block\n",
      "Enabled fused_attn for block\n",
      "Enabled fused_attn for block\n",
      "Enabled fused_attn for block\n",
      "Enabled fused_attn for block\n",
      "\n",
      "Model created with 12 blocks\n",
      "First block attention fused_attn: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_950524/3616865957.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
      "/gpfs/data/fieremanslab/dayne/miniconda3/envs/ssl-vision/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Forward pass successful with Flash Attention context!\n",
      "Output shape: torch.Size([2, 384])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Testing Flash Attention with actual model\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nBoth settings will be applied below (as they are in your training code)\")\n",
    "\n",
    "with initialize(version_base=None, config_path=\"configs\"):\n",
    "    cfg = compose(config_name=\"model/dino_v2\")\n",
    "    vit_cfg = cfg.model.vit\n",
    "\n",
    "    model = timm.create_model(\n",
    "        cfg.model.architecture,\n",
    "        pretrained=False,\n",
    "        num_classes=0,\n",
    "        img_size=vit_cfg.image_size,\n",
    "        patch_size=vit_cfg.patch_size,\n",
    "        embed_dim=vit_cfg.embed_dim,\n",
    "        depth=vit_cfg.depth,\n",
    "        num_heads=vit_cfg.num_heads,\n",
    "        mlp_ratio=vit_cfg.mlp_ratio,\n",
    "        drop_path_rate=vit_cfg.drop_path_rate,\n",
    "    )\n",
    "\n",
    "    # Force enable fused_attn for all attention blocks\n",
    "    for block in model.blocks:\n",
    "        if hasattr(block.attn, 'fused_attn'):\n",
    "            block.attn.fused_attn = True\n",
    "            print(f\"Enabled fused_attn for block\")\n",
    "\n",
    "    print(f\"\\nModel created with {len(model.blocks)} blocks\")\n",
    "    print(f\"First block attention fused_attn: {model.blocks[0].attn.fused_attn if hasattr(model.blocks[0].attn, 'fused_attn') else 'N/A'}\")\n",
    "\n",
    "    # Test forward pass to ensure it works\n",
    "    model = model.cuda()\n",
    "    x = torch.randn(2, 3, vit_cfg.image_size, vit_cfg.image_size).cuda()\n",
    "\n",
    "    with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "        with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_mem_efficient=False, enable_math=False):\n",
    "            try:\n",
    "                output = model(x)\n",
    "                print(f\"\\n✓ Forward pass successful with Flash Attention context!\")\n",
    "                print(f\"Output shape: {output.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\n✗ Forward pass failed: {e}\")\n",
    "                # Try with fallback\n",
    "                with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_mem_efficient=True, enable_math=True):\n",
    "                    output = model(x)\n",
    "                    print(f\"✓ Forward pass successful with fallback enabled\")\n",
    "                    print(f\"Output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e9de381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Benchmarking: Flash Attention vs Standard Attention\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_950524/855485967.py:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
      "/tmp/ipykernel_950524/855485967.py:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
      "/tmp/ipykernel_950524/855485967.py:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Test 1] With Flash Attention enforced:\n",
      "  Average time: 3.73 ms\n",
      "\n",
      "[Test 2] With standard math attention only:\n",
      "  Average time: 5.88 ms\n",
      "\n",
      "================================================================================\n",
      "✓ Flash Attention IS WORKING! Speedup: 1.58x faster\n",
      "  Flash Attention: 3.73 ms\n",
      "  Standard Math:   5.88 ms\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Benchmark to verify Flash Attention is actually being used\n",
    "print(\"=\" * 80)\n",
    "print(\"Benchmarking: Flash Attention vs Standard Attention\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import time\n",
    "\n",
    "with initialize(version_base=None, config_path=\"configs\"):\n",
    "    cfg = compose(config_name=\"model/dino_v2\")\n",
    "    vit_cfg = cfg.model.vit\n",
    "\n",
    "    model = timm.create_model(\n",
    "        cfg.model.architecture,\n",
    "        pretrained=False,\n",
    "        num_classes=0,\n",
    "        img_size=vit_cfg.image_size,\n",
    "        patch_size=vit_cfg.patch_size,\n",
    "        embed_dim=vit_cfg.embed_dim,\n",
    "        depth=vit_cfg.depth,\n",
    "        num_heads=vit_cfg.num_heads,\n",
    "        mlp_ratio=vit_cfg.mlp_ratio,\n",
    "        drop_path_rate=vit_cfg.drop_path_rate,\n",
    "    ).cuda()\n",
    "\n",
    "    # Enable fused attention\n",
    "    for block in model.blocks:\n",
    "        if hasattr(block.attn, 'fused_attn'):\n",
    "            block.attn.fused_attn = True\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Create test input\n",
    "    batch_size = 16\n",
    "    x = torch.randn(batch_size, 3, vit_cfg.image_size, vit_cfg.image_size).cuda()\n",
    "\n",
    "    # Warmup\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "        for _ in range(5):\n",
    "            _ = model(x)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # Test 1: With Flash Attention\n",
    "    print(\"\\n[Test 1] With Flash Attention enforced:\")\n",
    "    times_flash = []\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "        with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_mem_efficient=False, enable_math=False):\n",
    "            for i in range(20):\n",
    "                torch.cuda.synchronize()\n",
    "                start = time.time()\n",
    "                output = model(x)\n",
    "                torch.cuda.synchronize()\n",
    "                times_flash.append(time.time() - start)\n",
    "\n",
    "    avg_flash = sum(times_flash[5:]) / len(times_flash[5:])  # Skip first 5 for warmup\n",
    "    print(f\"  Average time: {avg_flash*1000:.2f} ms\")\n",
    "\n",
    "    # Test 2: With standard math attention only\n",
    "    print(\"\\n[Test 2] With standard math attention only:\")\n",
    "    times_math = []\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "        with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True):\n",
    "            for i in range(20):\n",
    "                torch.cuda.synchronize()\n",
    "                start = time.time()\n",
    "                output = model(x)\n",
    "                torch.cuda.synchronize()\n",
    "                times_math.append(time.time() - start)\n",
    "\n",
    "    avg_math = sum(times_math[5:]) / len(times_math[5:])\n",
    "    print(f\"  Average time: {avg_math*1000:.2f} ms\")\n",
    "\n",
    "    # Compare\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    if avg_flash < avg_math:\n",
    "        speedup = avg_math / avg_flash\n",
    "        print(f\"✓ Flash Attention IS WORKING! Speedup: {speedup:.2f}x faster\")\n",
    "        print(f\"  Flash Attention: {avg_flash*1000:.2f} ms\")\n",
    "        print(f\"  Standard Math:   {avg_math*1000:.2f} ms\")\n",
    "    else:\n",
    "        print(f\"⚠ Flash Attention may not be working properly\")\n",
    "        print(f\"  Flash Attention: {avg_flash*1000:.2f} ms\")\n",
    "        print(f\"  Standard Math:   {avg_math*1000:.2f} ms\")\n",
    "    print(f\"{'='*80}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
